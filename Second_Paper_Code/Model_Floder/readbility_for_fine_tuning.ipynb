{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "from readability import Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['input', 'Human_output', 'instruction', 'messages',\n",
       "       'original_ouput_without_sys_prompt',\n",
       "       'modified_output_without_sys_prompt', 'not_fine_tuned_model_output',\n",
       "       'input_with_sys_prompt', 'original_ouput_with_sys_prompt',\n",
       "       'modified_output_with_sys_prompt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"combined_file_with_and_without_sys_prompt.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "BERTScore, a recently proposed automatic metric for machine translation quality, uses BERT, a large pre-trained language model to evaluate candidate translations with respect to a gold translation. Taking advantage of BERT’s semantic and syntactic abilities, BERTScore seeks to avoid the flaws of earlier approaches like BLEU, instead scoring candidate translations based on their semantic similarity to the gold sentence. However, BERT is not infallible; while its performance on NLP tasks set a new state of the art in general, studies of specific syntactic and semantic phenomena have shown where BERT’s performance deviates from that of humans more generally. This naturally raises the questions we address in this paper: what are the strengths and weaknesses of BERTScore? Do they relate to known weaknesses on the part of BERT? We find that while BERTScore can detect when a candidate differs from a reference in important content words, it is less sensitive to smaller errors, especially if the candidate is lexically or stylistically similar to the reference.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.2661\n"
     ]
    }
   ],
   "source": [
    "r = Readability(text)\n",
    "ans = r.flesch()\n",
    "score = round(ans.score, 4)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(text):\n",
    "    r = Readability(text)\n",
    "    ans = r.flesch()\n",
    "    score = round(ans.score, 4)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The readbility score for Human outputs: 63.6753\n",
      "\n",
      " The readbility score for Fine_tuned_model_with_ins_prompt_text: 79.821\n",
      "\n",
      " The readbility score for Fine_tuned_model_without_ins_prompt_text: 78.736\n",
      "\n",
      " The readbility score for Not_Fine_tuned_model_text: 74.3013\n"
     ]
    }
   ],
   "source": [
    "Human_output_text = ' '.join(df['Human_output'].astype(str).tolist())\n",
    "Fine_tuned_model_with_ins_prompt_text = \" \".join(df['modified_output_with_sys_prompt'].astype(str).tolist())\n",
    "Fine_tuned_model_without_ins_prompt_text = \" \".join(df['modified_output_without_sys_prompt'].astype(str).tolist())\n",
    "Not_Fine_tuned_model_text = \" \".join(df['not_fine_tuned_model_output'].astype(str).tolist())\n",
    "\n",
    "\n",
    "Human_output_text_read_score = read(Human_output_text)\n",
    "Fine_tuned_model_with_ins_prompt_output_text_read_score = read(Fine_tuned_model_with_ins_prompt_text)\n",
    "Fine_tuned_model_without_ins_prompt_output_text_read_score = read(Fine_tuned_model_without_ins_prompt_text)\n",
    "Not_Fine_tuned_model_output_text_read_score = read(Not_Fine_tuned_model_text)\n",
    "\n",
    "print(f\"\\n The readbility score for Human outputs: {Human_output_text_read_score}\")\n",
    "print(f\"\\n The readbility score for Fine_tuned_model_with_ins_prompt_text: {Fine_tuned_model_with_ins_prompt_output_text_read_score}\")\n",
    "print(f\"\\n The readbility score for Fine_tuned_model_without_ins_prompt_text: {Fine_tuned_model_without_ins_prompt_output_text_read_score}\")\n",
    "print(f\"\\n The readbility score for Not_Fine_tuned_model_text: {Not_Fine_tuned_model_output_text_read_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
